{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "P75gWMshhfv_",
   "metadata": {
    "id": "P75gWMshhfv_"
   },
   "source": [
    "# Начальная инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7bda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-March-2023 17:51:31\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65078f9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65078f9a",
    "outputId": "0b40f77d-a829-4f88-8e9d-83f02bf0e4b5"
   },
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "# !pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "# !pip install scikit-learn\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch.optim as optim\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import re\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from string import punctuation\n",
    "\n",
    "# %pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qZMoTvwu7wGy",
   "metadata": {
    "id": "qZMoTvwu7wGy"
   },
   "outputs": [],
   "source": [
    "# Подключение вычислений на видеокарту, если доступна\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GnUj7tRT96cu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnUj7tRT96cu",
    "outputId": "c0f3c9ff-9d0e-4157-e914-4d1220d0e00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IVxnHGLRdEAF",
   "metadata": {
    "id": "IVxnHGLRdEAF"
   },
   "source": [
    "# Word2vec-признаки, обученные самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd753b2e",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699c8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('X_y_train.xlsx')\n",
    "test = pd.read_excel('X_y_test.xlsx')\n",
    "\n",
    "X_train = [el[0] for el in train[['Text']].values]\n",
    "X_test = [el[0] for el in test[['Text']].values]\n",
    "y_train = [el[0] for el in train[['Class']].replace(-1, 0).values]\n",
    "y_test = [el[0] for el in test[['Class']].replace(-1, 0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f876fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a X_token file one time!\n",
    "\n",
    "# def tokenize(text):\n",
    "\n",
    "#   text_token = nltk.word_tokenize(text)\n",
    "#   text_word = [el.lower() for el in text_token if el not in punctuation]\n",
    "#   return text_word\n",
    "\n",
    "# X_train_token = [tokenize(t) for t in X_train]\n",
    "# X_test_token = [tokenize(t) for t in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fee7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_train_token.txt', mode='w+') as file:\n",
    "#     for sentence in X_train_token:\n",
    "#         print(*sentence, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a57a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token = [sentence.split() for sentence in open('X_token.txt', mode='r')]\n",
    "X_train_token = [sentence.split() for sentence in open('X_train_token.txt', mode='r')]\n",
    "X_test_token = [sentence.split() for sentence in open('X_test_token.txt', mode='r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d43f1",
   "metadata": {},
   "source": [
    "## Обучение модели word2vec на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b4db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=X_token, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deeed725",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a500772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78023432, 92066100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(X_token, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n",
    "# w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = w2v_model.wv['работа']\n",
    "# print(vector)\n",
    "w2v_model.save('self-trained_word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6240e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(model, words):\n",
    "    all_words, mean = set(), []\n",
    "\n",
    "    for word in words:\n",
    "        mean.append(model.wv[word])\n",
    "        all_words.add(word)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a68a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_list(model, text_list):\n",
    "    return np.vstack([word_averaging(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc155ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "X_train = word_averaging_list(w2v_model, X_train_token)\n",
    "X_test = word_averaging_list(w2v_model, X_test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4a2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacea138",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b37b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mB-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce373c84",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net = None, learning_rate = None, x = None, y = None, batch = None, epochs = None):\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    inputs_train = torch.tensor(x).to(device)\n",
    "    targets_train = torch.tensor(y).int().to(device)\n",
    "\n",
    "    train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "\n",
    "    trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batch)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        loss = None\n",
    "        for data in trainset:\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X.float())\n",
    "            loss = F.cross_entropy(output, y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "    return net\n",
    "\n",
    "def test_net(net = None, device = None, x = None, y = None):\n",
    "    inputs_test = torch.tensor(x).to(device)\n",
    "    targets_test = torch.tensor(y).int().to(device)\n",
    "    \n",
    "    test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "    testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1)\n",
    "\n",
    "    ams = []\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            output = net(X.float())\n",
    "            for idx, i in enumerate(output):\n",
    "                ams.append(torch.argmax(i).item())\n",
    "    return f1_score(y, ams, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d4b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc4 = nn.Linear(layers[2], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ea6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "# optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caea734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "\n",
    "\n",
    "# inputs_train = torch.tensor(X_train).to(device)\n",
    "# targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "# inputs_test = torch.tensor(X_test).to(device)\n",
    "# targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "# train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "# test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "# trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1)\n",
    "# testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d5d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7670, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(10): # 15 полных прохода по нашим данным\n",
    "#     for data in trainset:  # `data` это батч наших данных\n",
    "#         X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        \n",
    "#         net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "#         output = net_2_layer(X.float())  # передаем батч\n",
    "#         #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "#         loss = F.cross_entropy(output, y.long())\n",
    "#         loss.backward()  # передаем это значение назад по сети\n",
    "#         optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "#     print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab3878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b273a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.76        23\n",
      "    positive       0.79      0.81      0.80        27\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.78      0.78      0.78        50\n",
      "weighted avg       0.78      0.78      0.78        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee19a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391b6b6",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce4d4cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19eb8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b30df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c203fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        \n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18fd3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24f4d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85f9782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6bb7",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c818a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "049be045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3bc5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9d6c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        \n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81fa8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19c4ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fa4f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142fef2",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c598678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a59acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a3280fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcd3536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        \n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dffc1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9509cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.68      0.67     33421\n",
      "    positive       0.68      0.65      0.66     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.67     67817\n",
      "weighted avg       0.67      0.67      0.67     67817\n",
      "\n",
      "f1 = 0.6649504364550969\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4044f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feeb91f",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7edb9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51a9dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "607cada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de306c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        \n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72e241b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "654e0f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.59      0.63     33421\n",
      "    positive       0.65      0.72      0.68     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.66      0.66      0.66     67817\n",
      "weighted avg       0.66      0.66      0.66     67817\n",
      "\n",
      "f1 = 0.6832180500658762\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63b3840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d7e25",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "859d1e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c661191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57dc68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a568aca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6813e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c31186ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bde5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55feac5",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3aed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ece8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0be896a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b7e65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e4d00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "122cb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.70      0.67     33421\n",
      "    positive       0.68      0.63      0.66     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.67     67817\n",
      "weighted avg       0.67      0.67      0.67     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6dc8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e762c25",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a826fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8150ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ba320f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5db8b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f861fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9fa11a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.59      0.63     33421\n",
      "    positive       0.65      0.73      0.69     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.67      0.66      0.66     67817\n",
      "weighted avg       0.67      0.66      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d52199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0726f44",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c01dedc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c48a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c029810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d6650f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc6e9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "265e3507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee12f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b331e51",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00fd84a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa64201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0495ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67ea6105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94bf1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "459b2b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.68      0.67     33421\n",
      "    positive       0.68      0.65      0.66     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.67     67817\n",
      "weighted avg       0.67      0.67      0.67     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d99ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b98f8",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f88e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de33967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee693352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47a84aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9085315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07aece66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.61      0.64     33421\n",
      "    positive       0.65      0.70      0.68     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.66      0.66      0.66     67817\n",
      "weighted avg       0.66      0.66      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64c3f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21493b",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d2ac4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbc56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f90a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "976d9a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c69b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "baaca4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b113f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064b732",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a82c1dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "517583e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "39421cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e78f6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7edba273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6feb61ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.73      0.68     33421\n",
      "    positive       0.69      0.61      0.65     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.66     67817\n",
      "weighted avg       0.67      0.67      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "726f8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f953141",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4da4a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2e7e1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9be8f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c1dc9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a438c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45bf1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.64      0.65     33421\n",
      "    positive       0.66      0.67      0.66     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.66      0.66      0.66     67817\n",
      "weighted avg       0.66      0.66      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f696385",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31602562",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15a015d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "094a212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb36d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fbfe488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0a1e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08597f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5859f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0baaf3",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1adbefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7a7f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4fa9c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d56ccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e619be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "86c703b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.60      0.64     33421\n",
      "    positive       0.65      0.73      0.69     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.66     67817\n",
      "weighted avg       0.67      0.67      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a165f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3b4b9",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "45f7a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56b9bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "16cafdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6b668c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0ffdfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "637fb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.62      0.64     33421\n",
      "    positive       0.66      0.70      0.68     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.66      0.66      0.66     67817\n",
      "weighted avg       0.66      0.66      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3af0285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4aafb",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d11eff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "30aef4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8efe5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4cb00c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5ae0c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "09114d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1814942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "311f43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-March-2023 01:42:13\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207a99e",
   "metadata": {},
   "source": [
    "# FastText-признаки, обученные самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ef983",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "48ef0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv', sep=';')\n",
    "# df = pd.read_csv('df.csv', sep=';')\n",
    "\n",
    "X = [el[0] for el in df[['Текст']].values]\n",
    "Y = [el[0] for el in df[['Оценка']].replace(-1, 0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ded2e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an X_token file one time!\n",
    "\n",
    "# def tokenize(text):\n",
    "\n",
    "  # without_punct = re.sub(\"[^А-Яа-я\\sЁё]\", \"\", text) # Удаление пунктуации (оставление только русских букв и пробелов) \n",
    "  # without_punct = re.sub('\\s\\s', ' ', without_punct) # Удаление двойного пробела\n",
    "  # tokens = re.split(' ', without_punct) # Разделение на отдельные слова (токенизация)\n",
    "  # tokens = [word for word in tokens if word.isalnum()] # Отбор именно слов и чисел!!!\n",
    "  # tokens = [word.lower() for word in tokens if not word.isnumeric()] # Удаление чисел и приведение к нижнему регистру\n",
    "  # return tokens\n",
    "\n",
    "  # text_token = nltk.word_tokenize(text)\n",
    "  # text_word = [el.lower() for el in text_token if el not in punctuation]\n",
    "  # return text_word\n",
    "\n",
    "# X_token = [tokenize(t) for t in X]\n",
    "\n",
    "# with open('X_token.txt', mode='w+') as file:\n",
    "#     for sentence in X_token:\n",
    "#         print(*sentence, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e0e0ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token = [sentence.split() for sentence in open('X_token.txt', mode='r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ac745",
   "metadata": {},
   "source": [
    "## Обучение модели FastText на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "693d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sentences=X_token, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1a5c97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.build_vocab(X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bad328dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388703321, 409069200)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.train(X_token, total_examples=ft_model.corpus_count, epochs=300, report_delay=1)\n",
    "# ft_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7c9e6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ft_model.wv['работа']\n",
    "# print(vector)\n",
    "ft_model.save('self-trained_fasttext/fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "45201f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(model, words):\n",
    "  all_words, mean = set(), []\n",
    "\n",
    "  for word in words:\n",
    "    mean.append(ft_model.wv[word])\n",
    "    all_words.add(word)\n",
    "\n",
    "  if not mean:\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "  mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "  return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "47ff549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_list(model, text_list):\n",
    "  return np.vstack([word_averaging(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1ef21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = ft_model.wv\n",
    "X = word_averaging_list(ft_model, X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "42f3ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b5a5e",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "de7dde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb41c1",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "799511a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a1c6ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d327331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dbfcd8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4554, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f958dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ca27481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.64      0.66     33421\n",
      "    positive       0.67      0.71      0.69     34396\n",
      "\n",
      "    accuracy                           0.68     67817\n",
      "   macro avg       0.68      0.68      0.68     67817\n",
      "weighted avg       0.68      0.68      0.68     67817\n",
      "\n",
      "f1 = 0.6916644338837731\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b261e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8b686",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4b8e06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "da9478ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e0d11b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2bbfa190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9b25205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "385ebae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.64      0.65     33421\n",
      "    positive       0.66      0.68      0.67     34396\n",
      "\n",
      "    accuracy                           0.66     67817\n",
      "   macro avg       0.66      0.66      0.66     67817\n",
      "weighted avg       0.66      0.66      0.66     67817\n",
      "\n",
      "f1 = 0.6670010173814608\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b98deae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40c2f5",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d062bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "215d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2d1eda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1f5f941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0fed23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c2c5dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "19fd92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcd53d",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2b027637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cc56306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "921d318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "03d2190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1d2469a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f1082740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.66      0.67     33421\n",
      "    positive       0.68      0.69      0.68     34396\n",
      "\n",
      "    accuracy                           0.68     67817\n",
      "   macro avg       0.68      0.68      0.68     67817\n",
      "weighted avg       0.68      0.68      0.68     67817\n",
      "\n",
      "f1 = 0.6840523871821044\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f18742e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373643dd",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "00c13f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "898a2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "000cc7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "41e3a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "91d6c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9d8fe14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.66      0.66     33421\n",
      "    positive       0.67      0.68      0.68     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.67     67817\n",
      "weighted avg       0.67      0.67      0.67     67817\n",
      "\n",
      "f1 = 0.6754539030615924\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "73a39223",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc7b48",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4dd6e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bb4e762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fb1b21bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6d214276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8a4fe646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "283001ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.57      0.50      0.34     67817\n",
      "weighted avg       0.57      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.673001830085828\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0e1d1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90fbbd5",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1e274ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "744a1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "726d7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "98996ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2edbfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1d6560bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.69      0.68     33421\n",
      "    positive       0.69      0.67      0.68     34396\n",
      "\n",
      "    accuracy                           0.68     67817\n",
      "   macro avg       0.68      0.68      0.68     67817\n",
      "weighted avg       0.68      0.68      0.68     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "216c7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39db934",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "83446ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "211f7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e9efd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e598e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4a0f7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9afaa172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "059eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb347e",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "eb75f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c2a4bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "41894157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "47ab3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5333fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4a6dd44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "17ae6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d726c68",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "71ef6907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dc90ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f5a6448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "107cac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "772912b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "02f5eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.65      0.66     33421\n",
      "    positive       0.67      0.71      0.69     34396\n",
      "\n",
      "    accuracy                           0.68     67817\n",
      "   macro avg       0.68      0.68      0.68     67817\n",
      "weighted avg       0.68      0.68      0.68     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c4a09aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746099",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7a423d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ed2723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "05dcec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bf91ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2ba51012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e45c67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.61      0.64     33421\n",
      "    positive       0.65      0.72      0.69     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.67      0.67      0.66     67817\n",
      "weighted avg       0.67      0.67      0.66     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7a02e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d1e39",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ceab4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "114729d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3f9f1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "12fbdea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "99532115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c15bd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "870af2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc59ce",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dbf090a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "411a9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "65383d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4434bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "44956a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0fe96ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.57      0.63     33421\n",
      "    positive       0.65      0.77      0.71     34396\n",
      "\n",
      "    accuracy                           0.68     67817\n",
      "   macro avg       0.68      0.67      0.67     67817\n",
      "weighted avg       0.68      0.68      0.67     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "643b64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42e921",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ad4cb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "db6a2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1023834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0ef37f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8e7dde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3f5bcc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "21fa2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966a4ea",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "29531f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "38d2ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7dbd7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "db2109c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ee3fa80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c2f0a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c0476059",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27f63f",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "dcdc8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6e8c22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "73a23eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "8f6a5996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "05a989ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f5c01f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.54      0.62     33421\n",
      "    positive       0.64      0.80      0.71     34396\n",
      "\n",
      "    accuracy                           0.67     67817\n",
      "   macro avg       0.68      0.67      0.67     67817\n",
      "weighted avg       0.68      0.67      0.67     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1ce893e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675bd3f",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ed1f393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c298b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4c342ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "784e9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f37c72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dfd3fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1bddf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4382fc7",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f0c93192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "98679028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c6fcb0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d32ac339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3faacb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "de791503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e14d9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_self_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8f6bbf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-March-2023 04:59:46\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e77b08",
   "metadata": {},
   "source": [
    "# Word2Vec-признаки предобученные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098e93e",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f0f83dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv', sep=';')\n",
    "\n",
    "X = [el[0] for el in df[['Текст']].values]\n",
    "Y = [el[0] for el in df[['Оценка']].replace(-1, 0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fe22f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages (3.2)\n",
      "Requirement already satisfied: ufal.udpipe in /home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages (1.3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "!pip install wget\n",
    "import wget\n",
    "import re\n",
    "!pip install ufal.udpipe\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_ADJ трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = \"x\" * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(\" \", \"\")\n",
    "    if token == \"Файл\" and \"SpaceAfter=No\" in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "    if \"|\" in out_lemma or out_lemma.endswith(\".jpg\") or out_lemma.endswith(\".png\"):\n",
    "        return None\n",
    "    if pos != \"PUNCT\":\n",
    "        if out_lemma.startswith(\"«\") or out_lemma.startswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[1:])\n",
    "        if out_lemma.endswith(\"«\") or out_lemma.endswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "        if (\n",
    "            out_lemma.endswith(\"!\")\n",
    "            or out_lemma.endswith(\"?\")\n",
    "            or out_lemma.endswith(\",\")\n",
    "            or out_lemma.endswith(\".\")\n",
    "        ):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace(\n",
    "        \"\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019\",\n",
    "        \"\\u0022\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF\", \"\\u2003\\u002D\\u002D\\u2003\", text\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2010\\u2011\", \"\\u002D\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000\",\n",
    "        \"\\u2002\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = re.sub(\"\\u2003\\u2003\", \"\\u2003\", text)\n",
    "    text = re.sub(\"\\t\\t\", \"\\t\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062\",\n",
    "        \".\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2217\", \"\\u002A\", text)\n",
    "\n",
    "    text = list_replace(\"…\", \"...\", text)\n",
    "\n",
    "    text = list_replace(\"\\u2241\\u224B\\u2E2F\\u0483\", \"\\u223D\", text)\n",
    "\n",
    "    text = list_replace(\"\\u00C4\", \"A\", text)  # латинская\n",
    "    text = list_replace(\"\\u00E4\", \"a\", text)\n",
    "    text = list_replace(\"\\u00CB\", \"E\", text)\n",
    "    text = list_replace(\"\\u00EB\", \"e\", text)\n",
    "    text = list_replace(\"\\u1E26\", \"H\", text)\n",
    "    text = list_replace(\"\\u1E27\", \"h\", text)\n",
    "    text = list_replace(\"\\u00CF\", \"I\", text)\n",
    "    text = list_replace(\"\\u00EF\", \"i\", text)\n",
    "    text = list_replace(\"\\u00D6\", \"O\", text)\n",
    "    text = list_replace(\"\\u00F6\", \"o\", text)\n",
    "    text = list_replace(\"\\u00DC\", \"U\", text)\n",
    "    text = list_replace(\"\\u00FC\", \"u\", text)\n",
    "    text = list_replace(\"\\u0178\", \"Y\", text)\n",
    "    text = list_replace(\"\\u00FF\", \"y\", text)\n",
    "    text = list_replace(\"\\u00DF\", \"s\", text)\n",
    "    text = list_replace(\"\\u1E9E\", \"S\", text)\n",
    "\n",
    "    currencies = list(\n",
    "        \"\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192\"\n",
    "    )\n",
    "\n",
    "    alphabet = list(\n",
    "        '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
    "    )\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = \"\".join(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def process(pipeline, text=\"Строка\", keep_pos=True, keep_punct=False):\n",
    "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "    entities = {\"PROPN\"}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [line for line in processed.split(\"\\n\") if not line.startswith(\"#\")]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split(\"\\t\") for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if \"|\" not in feats:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split(\"=\")[0]: el.split(\"=\")[1] for el in feats.split(\"|\")}\n",
    "            if \"Case\" not in morph or \"Number\" not in morph:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph[\"Case\"]\n",
    "                mem_number = morph[\"Number\"]\n",
    "            if morph[\"Case\"] == mem_case and morph[\"Number\"] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if \"SpacesAfter=\\\\n\" in misc or \"SpacesAfter=\\s\\\\n\" in misc:\n",
    "                    named = False\n",
    "                    past_lemma = \"::\".join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if (\n",
    "                    pos == \"NUM\" and token.isdigit()\n",
    "                ):  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split(\"_\")[1] != \"PUNCT\"]\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split(\"_\")[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = \"https://rusvectores.org/static/models/udpipe_syntagrus.model\"\n",
    "udpipe_filename = '' + udpipe_model_url.split(\"/\")[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print(\"UDPipe model not found. Downloading...\", file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print(\"\\nLoading the model...\", file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(\n",
    "    model, \"tokenize\", Pipeline.DEFAULT, Pipeline.DEFAULT, \"conllu\"\n",
    ")\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "X_token = []\n",
    "for input_line in df['Текст']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res)\n",
    "    X_token.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac7ac6",
   "metadata": {},
   "source": [
    "## Обучение модели Word2Vec на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "70cfe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec/model.bin', encoding='utf-8', unicode_errors='ignore', binary=True)\n",
    "w2v_model.fill_norms(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8d4acd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.02568102e+00  4.10510159e+00  3.96478510e+00  1.99146974e+00\n",
      "  2.75750041e-01  1.05907798e+00 -2.34842032e-01  1.13069057e+00\n",
      " -3.34521389e+00  5.44947767e+00 -1.78760219e+00  2.93239641e+00\n",
      "  4.24604845e+00 -3.08241105e+00 -3.06380081e+00  4.14739418e+00\n",
      "  1.94640350e+00 -6.41723156e+00  4.48100597e-01  1.98949501e-01\n",
      " -1.96533740e+00  2.04884505e+00  6.81714356e-01  1.89991868e+00\n",
      " -1.87503803e+00  1.61289966e+00  8.80351245e-01  1.27756655e+00\n",
      " -1.60905108e-01 -2.56419826e+00  5.59642196e-01 -2.38538122e+00\n",
      "  5.61529458e-01 -3.78697932e-01 -5.17279339e+00 -4.75222081e-01\n",
      " -7.38774776e-01 -1.47579443e+00  5.36987162e+00  1.66592920e+00\n",
      "  2.48067904e+00 -3.41140532e+00 -3.26146185e-01  1.99449539e+00\n",
      "  1.23095262e+00  1.58154178e+00  1.74632573e+00  1.36116230e+00\n",
      " -2.66357565e+00  6.55228496e-01  2.66324496e+00 -2.19835329e+00\n",
      " -7.47147948e-02  2.37323642e+00 -2.46333432e+00  2.09079415e-01\n",
      " -2.15091515e+00 -5.49663115e+00 -8.21452737e-01  1.57285839e-01\n",
      "  6.61701798e-01 -3.56726438e-01 -3.01842904e+00  7.32664943e-01\n",
      "  2.08598614e+00 -1.41081357e+00 -5.02162099e-01 -3.44310713e+00\n",
      " -2.70832729e+00 -1.67359900e+00 -4.65843487e+00  4.59569216e+00\n",
      " -3.65738130e+00 -1.68344200e+00  1.12880623e+00 -2.32587099e+00\n",
      "  2.25881290e+00 -5.71228564e-01 -1.42057741e+00 -9.63704064e-02\n",
      " -7.41252661e-01  2.72428006e-01  5.86827815e-01 -1.91268504e+00\n",
      " -1.54868975e-01  5.33265948e-01 -9.93554652e-01  1.19762135e+00\n",
      "  1.97423756e+00 -2.44773769e+00 -4.29723382e-01 -2.19450855e+00\n",
      " -8.78581703e-01 -3.68831038e-01 -4.73884058e+00 -9.95291829e-01\n",
      "  1.96129644e+00 -2.23322058e+00 -1.35348892e+00  1.84871805e+00\n",
      "  2.98968244e+00  4.03689671e+00 -1.15203686e-01  1.97898507e+00\n",
      "  2.20201778e+00  2.63293362e+00 -1.79020613e-01  1.63138545e+00\n",
      " -7.25692570e-01  2.64699012e-01 -1.40380383e+00  1.80271244e+00\n",
      " -5.17523718e+00  4.29810143e+00 -1.96248770e+00 -1.80293024e-01\n",
      "  4.80625749e-01 -2.52803373e+00  9.15996060e-02 -1.33048618e+00\n",
      " -9.11542594e-01  3.21305346e+00  2.29246783e+00  1.28174770e+00\n",
      "  9.81810153e-01 -3.35683489e+00  3.94806170e+00  2.68293023e-01\n",
      " -4.58949536e-01 -1.18627512e+00  6.55168951e-01 -6.76943883e-02\n",
      " -2.29546592e-01 -2.00870585e+00  3.66676283e+00  1.87209797e+00\n",
      " -1.45867634e+00 -2.36221835e-01  3.55628419e+00 -1.16016120e-01\n",
      "  9.67946708e-01  7.77125597e-01  1.11526275e+00 -2.28089309e+00\n",
      "  8.53556693e-01  1.52941298e+00 -7.59864867e-01 -2.07443190e+00\n",
      "  1.91821420e+00  2.16583580e-01 -3.45948243e+00  2.75355172e+00\n",
      " -3.14419675e+00  3.06619215e+00  2.17322350e+00 -8.57929528e-01\n",
      "  3.04637694e+00  2.34819007e+00  1.03708768e+00 -2.49978733e+00\n",
      " -3.83608341e+00 -3.33338594e+00  3.34870982e+00 -1.91703122e-02\n",
      "  1.28278577e+00 -2.13554525e+00  8.76425862e-01  1.94094157e+00\n",
      "  1.48071671e+00 -8.53495061e-01 -8.66016686e-01  3.01546168e+00\n",
      "  4.24282014e-01 -5.61698258e-01 -1.72610307e+00  4.98404074e+00\n",
      " -2.34956574e+00  7.54057050e-01  6.40150642e+00 -4.51727629e+00\n",
      "  7.40165651e-01 -8.57133865e-01 -3.01201391e+00 -2.03801298e+00\n",
      " -4.95920330e-01 -4.75771904e+00  1.89506114e+00  2.62465096e+00\n",
      " -1.93081462e+00 -3.10070252e+00 -5.64702928e-01  3.94759560e+00\n",
      " -6.75520897e-01 -5.14421701e-01 -2.28597283e+00 -5.10558891e+00\n",
      " -1.69497812e+00 -4.99807626e-01 -4.22593355e+00 -3.29628015e+00\n",
      " -3.64795351e+00  4.27742064e-01  1.55653608e+00 -1.20746028e+00\n",
      "  1.16968453e+00 -4.12662745e+00  6.82037175e-01 -6.08240008e-01\n",
      "  3.56517601e+00 -2.82430887e+00 -2.33061457e+00  1.25689173e+00\n",
      "  2.00057721e+00 -1.98435462e+00  2.28040171e+00 -4.27699947e+00\n",
      " -7.83862472e-01 -6.97373772e+00 -3.73213589e-01 -2.76460260e-01\n",
      "  1.53282499e+00 -5.59267163e-01 -2.47404599e+00 -4.66384125e+00\n",
      "  1.36715043e+00  1.13244104e+00  1.92361784e+00 -3.44760823e+00\n",
      " -1.85811722e+00 -5.99661022e-02 -1.47647813e-01  3.33556509e+00\n",
      " -1.07680261e+00 -2.74878240e+00 -1.08012426e+00  4.64338350e+00\n",
      " -1.33808827e+00  1.01821266e-01  1.19078994e+00 -3.83346200e+00\n",
      " -2.95207191e+00  4.63828325e-01  3.74571919e-01 -1.87971961e+00\n",
      "  4.65013683e-01 -2.98640633e+00  1.85878265e+00  1.87133837e+00\n",
      " -1.41013324e+00 -1.76775730e+00 -1.67912006e+00  6.38692498e-01\n",
      "  6.00675154e+00  1.78430486e+00  5.12833929e+00  1.16797268e+00\n",
      "  7.20571876e-01 -7.98638880e-01 -1.85424268e+00 -3.97217155e-01\n",
      "  2.13334227e+00  1.77917492e+00  2.41807723e+00  2.63354468e+00\n",
      "  8.88301134e-02  1.47717535e+00  4.51532936e+00  2.33813906e+00\n",
      " -1.32290244e+00 -1.43293786e+00  2.42255464e-01  2.47964597e+00\n",
      "  9.54133809e-01 -3.64849389e-01  4.55634773e-01 -1.37747264e+00\n",
      " -2.67389506e-01 -3.57467103e+00  1.96309328e+00  1.32332194e+00\n",
      "  2.98427701e+00  4.46703047e-01 -1.31625247e+00 -7.52604663e-01\n",
      "  1.47587705e+00 -5.63624322e-01 -3.61198211e+00  5.86269081e-01\n",
      "  3.10935116e+00 -3.36393505e-01  1.92764625e-01  3.11378855e-03\n",
      " -9.74766254e-01  9.20407593e-01  1.70369864e+00  1.72775781e+00\n",
      "  1.49772477e+00 -5.23070872e-01  1.70956588e+00  1.36436200e+00]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_model[w2v_model.key_to_index['работа_NOUN']]\n",
    "print(vector)\n",
    "# w2v_model.save('fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "60e1b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(model, words):\n",
    "  all_words, mean = set(), []\n",
    "\n",
    "  for word in words:\n",
    "    if word in model.key_to_index:\n",
    "      mean.append(model[model.key_to_index[word]])\n",
    "      all_words.add(word)\n",
    "\n",
    "  if not mean:\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "  mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "  return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "719e6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_list(model, text_list):\n",
    "  return np.vstack([word_averaging(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a5822ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = w2v_model[:]\n",
    "X = word_averaging_list(w2v_model, X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "384e9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b4db6",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1822139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad3544",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fa5c97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "eb43004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5c3f196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "045c7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c1237b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "462a992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.76      0.65     33421\n",
      "    positive       0.65      0.45      0.53     34396\n",
      "\n",
      "    accuracy                           0.60     67817\n",
      "   macro avg       0.61      0.60      0.59     67817\n",
      "weighted avg       0.61      0.60      0.59     67817\n",
      "\n",
      "f1 = 0.5311814965047036\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3ec5f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade880fd",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7716371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "86c0be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7d456bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ba8752b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "931141a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "31ca5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "96cf56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f37b88",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1067907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4c36feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "aed8022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6f22770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "71ac78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1b7f5eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ae903838",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ed816",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "26bed94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "00e610ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "acad7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c76928bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "821bcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d6d5bab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.76      0.65     33421\n",
      "    positive       0.65      0.44      0.52     34396\n",
      "\n",
      "    accuracy                           0.60     67817\n",
      "   macro avg       0.61      0.60      0.59     67817\n",
      "weighted avg       0.61      0.60      0.59     67817\n",
      "\n",
      "f1 = 0.5246670255439596\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8c1e112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f77c5e",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "97978027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "06a2bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f3a819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e9f183cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "210f3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dc5ccc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.51      0.55     33421\n",
      "    positive       0.58      0.67      0.62     34396\n",
      "\n",
      "    accuracy                           0.59     67817\n",
      "   macro avg       0.59      0.59      0.58     67817\n",
      "weighted avg       0.59      0.59      0.58     67817\n",
      "\n",
      "f1 = 0.6211864751046053\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "2df28921",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9429ab",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "956f2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "efbb9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "0560b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "772cc22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "4f051743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6a8ab774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "ade25b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a5404",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "1a341f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d470dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4477516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "513bb183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "1d1c19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "0c311633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.64      0.61     33421\n",
      "    positive       0.62      0.58      0.60     34396\n",
      "\n",
      "    accuracy                           0.61     67817\n",
      "   macro avg       0.61      0.61      0.61     67817\n",
      "weighted avg       0.61      0.61      0.61     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "915d3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d659bc1",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d8dcd956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "34129391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2c756f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a4428542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c7a186e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "354de2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.66      0.62     33421\n",
      "    positive       0.62      0.53      0.57     34396\n",
      "\n",
      "    accuracy                           0.59     67817\n",
      "   macro avg       0.60      0.60      0.59     67817\n",
      "weighted avg       0.60      0.59      0.59     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "5fcb8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30dc03",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "310097fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e5efb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "3df7531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c8e6204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "eb9c3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "d8ca57f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c328c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494e44d",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b88b5c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8ca8ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fe2234a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a245b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c5196873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "2dadd8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.67      0.63     33421\n",
      "    positive       0.63      0.54      0.58     34396\n",
      "\n",
      "    accuracy                           0.61     67817\n",
      "   macro avg       0.61      0.61      0.60     67817\n",
      "weighted avg       0.61      0.61      0.60     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8066a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a5ea7",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ea330f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e66dd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c95b2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b1708e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "eaa30fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "de35101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.50      0.55     33421\n",
      "    positive       0.58      0.68      0.63     34396\n",
      "\n",
      "    accuracy                           0.59     67817\n",
      "   macro avg       0.60      0.59      0.59     67817\n",
      "weighted avg       0.59      0.59      0.59     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "ecdb97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed777b",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "bf2ed337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "296c0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "42e30b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "31c7b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "45774b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "3184ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "755f1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7d3eb",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "69362046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "93bf9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "759b3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c0451a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "5404cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ea6930b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.67      0.63     33421\n",
      "    positive       0.63      0.55      0.59     34396\n",
      "\n",
      "    accuracy                           0.61     67817\n",
      "   macro avg       0.61      0.61      0.61     67817\n",
      "weighted avg       0.61      0.61      0.61     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d7ee21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00003652",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f8faa940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "9c5db576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c03f964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f86eeac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b36dd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "e66ca7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a1fb2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2882c1",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "2cf68d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "03c2aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3578d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "cd8b919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "2c4af746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "bd1ba24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "7bdf1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f5f88",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "09ccb8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c215f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "9733d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "4c16935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1e31d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2ede40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.50      0.56     33421\n",
      "    positive       0.59      0.71      0.65     34396\n",
      "\n",
      "    accuracy                           0.61     67817\n",
      "   macro avg       0.61      0.60      0.60     67817\n",
      "weighted avg       0.61      0.61      0.60     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "05d84210",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd0440",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "689b3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0983e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "3fa475c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "10c8edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "4f4f0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8eacfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f592712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faae8bf",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a0d6672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2b239ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "34b0c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "6b5586aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "adca95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "3dea9aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "356707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_w2v_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "62e79e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-March-2023 08:05:08\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52ff52",
   "metadata": {},
   "source": [
    "# FastText-признаки предобученные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3d699",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "70d470a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv', sep=';')\n",
    "\n",
    "X = [el[0] for el in df[['Текст']].values]\n",
    "Y = [el[0] for el in df[['Оценка']].replace(-1, 0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "db667e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_ADJ трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = \"x\" * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(\" \", \"\")\n",
    "    if token == \"Файл\" and \"SpaceAfter=No\" in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "    if \"|\" in out_lemma or out_lemma.endswith(\".jpg\") or out_lemma.endswith(\".png\"):\n",
    "        return None\n",
    "    if pos != \"PUNCT\":\n",
    "        if out_lemma.startswith(\"«\") or out_lemma.startswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[1:])\n",
    "        if out_lemma.endswith(\"«\") or out_lemma.endswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "        if (\n",
    "            out_lemma.endswith(\"!\")\n",
    "            or out_lemma.endswith(\"?\")\n",
    "            or out_lemma.endswith(\",\")\n",
    "            or out_lemma.endswith(\".\")\n",
    "        ):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace(\n",
    "        \"\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019\",\n",
    "        \"\\u0022\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF\", \"\\u2003\\u002D\\u002D\\u2003\", text\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2010\\u2011\", \"\\u002D\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000\",\n",
    "        \"\\u2002\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = re.sub(\"\\u2003\\u2003\", \"\\u2003\", text)\n",
    "    text = re.sub(\"\\t\\t\", \"\\t\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062\",\n",
    "        \".\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2217\", \"\\u002A\", text)\n",
    "\n",
    "    text = list_replace(\"…\", \"...\", text)\n",
    "\n",
    "    text = list_replace(\"\\u2241\\u224B\\u2E2F\\u0483\", \"\\u223D\", text)\n",
    "\n",
    "    text = list_replace(\"\\u00C4\", \"A\", text)  # латинская\n",
    "    text = list_replace(\"\\u00E4\", \"a\", text)\n",
    "    text = list_replace(\"\\u00CB\", \"E\", text)\n",
    "    text = list_replace(\"\\u00EB\", \"e\", text)\n",
    "    text = list_replace(\"\\u1E26\", \"H\", text)\n",
    "    text = list_replace(\"\\u1E27\", \"h\", text)\n",
    "    text = list_replace(\"\\u00CF\", \"I\", text)\n",
    "    text = list_replace(\"\\u00EF\", \"i\", text)\n",
    "    text = list_replace(\"\\u00D6\", \"O\", text)\n",
    "    text = list_replace(\"\\u00F6\", \"o\", text)\n",
    "    text = list_replace(\"\\u00DC\", \"U\", text)\n",
    "    text = list_replace(\"\\u00FC\", \"u\", text)\n",
    "    text = list_replace(\"\\u0178\", \"Y\", text)\n",
    "    text = list_replace(\"\\u00FF\", \"y\", text)\n",
    "    text = list_replace(\"\\u00DF\", \"s\", text)\n",
    "    text = list_replace(\"\\u1E9E\", \"S\", text)\n",
    "\n",
    "    currencies = list(\n",
    "        \"\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192\"\n",
    "    )\n",
    "\n",
    "    alphabet = list(\n",
    "        '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
    "    )\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = \"\".join(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def process(pipeline, text=\"Строка\", keep_pos=True, keep_punct=False):\n",
    "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "    entities = {\"PROPN\"}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [line for line in processed.split(\"\\n\") if not line.startswith(\"#\")]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split(\"\\t\") for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if \"|\" not in feats:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split(\"=\")[0]: el.split(\"=\")[1] for el in feats.split(\"|\")}\n",
    "            if \"Case\" not in morph or \"Number\" not in morph:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph[\"Case\"]\n",
    "                mem_number = morph[\"Number\"]\n",
    "            if morph[\"Case\"] == mem_case and morph[\"Number\"] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if \"SpacesAfter=\\\\n\" in misc or \"SpacesAfter=\\s\\\\n\" in misc:\n",
    "                    named = False\n",
    "                    past_lemma = \"::\".join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if (\n",
    "                    pos == \"NUM\" and token.isdigit()\n",
    "                ):  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split(\"_\")[1] != \"PUNCT\"]\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split(\"_\")[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = \"https://rusvectores.org/static/models/udpipe_syntagrus.model\"\n",
    "udpipe_filename = '' + udpipe_model_url.split(\"/\")[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print(\"UDPipe model not found. Downloading...\", file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print(\"\\nLoading the model...\", file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(\n",
    "    model, \"tokenize\", Pipeline.DEFAULT, Pipeline.DEFAULT, \"conllu\"\n",
    ")\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "X_token = []\n",
    "for input_line in df['Текст']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res, keep_pos=False)\n",
    "    X_token.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f798ed",
   "metadata": {},
   "source": [
    "## Обучение модели FastText на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "1b1451cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = gensim.models.KeyedVectors.load('fasttext/model.model')\n",
    "ft_model.fill_norms(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "48c132b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.28143653e-01  6.31993353e-01  3.97833914e-01 -1.69036105e-01\n",
      "  1.94467843e-01  2.62616009e-01 -1.29946560e-01  2.99574584e-01\n",
      "  5.06506674e-02  5.92105603e-03  8.87992755e-02  4.69435602e-01\n",
      " -2.00550243e-01  6.64121136e-02  1.52241021e-01 -3.72883081e-01\n",
      " -3.08863670e-02 -1.04671396e-01  1.48081467e-01 -8.00064430e-02\n",
      "  7.97154009e-02 -1.13625549e-01 -3.49553585e-01 -1.42057359e-01\n",
      "  7.67205238e-01 -1.68227687e-01 -1.12291731e-01  3.17529649e-01\n",
      " -3.59128028e-01 -6.41788542e-02 -5.57220131e-02  1.65108591e-01\n",
      "  1.66394070e-01  5.43315411e-01 -1.59994856e-01  2.17812255e-01\n",
      "  2.80040950e-01 -5.70807010e-02  1.74972832e-01  1.98939666e-01\n",
      "  1.93150759e-01  2.96664566e-01  7.72891268e-02  5.42501509e-01\n",
      "  2.76810322e-02 -1.13098420e-01  1.08400442e-01  6.68987632e-03\n",
      " -1.36405602e-01 -9.27114263e-02 -5.01722358e-02 -3.12990457e-01\n",
      "  7.97539577e-02  3.24938953e-01 -1.21801049e-01  2.70672590e-01\n",
      " -4.02754359e-02  3.37236971e-01 -4.76787239e-01 -1.47199020e-01\n",
      "  8.80351439e-02 -2.75021195e-01 -6.54799163e-01 -8.04591656e-01\n",
      "  4.31546926e-01  2.73890465e-01 -1.09550454e-01  6.12769544e-01\n",
      " -2.65803132e-02  3.61043900e-01  3.85290504e-01  1.24445118e-01\n",
      " -3.23237568e-01  1.86322227e-01 -1.48882821e-01  2.89316565e-01\n",
      " -4.88742322e-01 -3.43459874e-01  9.79158878e-02  5.26423519e-03\n",
      "  7.48121142e-02 -1.55417221e-02 -1.04115635e-01 -1.79251507e-01\n",
      "  3.09590220e-01  1.14156574e-01  1.82896614e-01 -5.79745471e-01\n",
      " -3.30249161e-01  1.98455855e-01 -2.51652986e-01 -2.51863408e-03\n",
      "  4.93122250e-01  2.09248379e-01 -1.62874639e-01  2.51050502e-01\n",
      "  3.56166512e-01  6.87424466e-02 -9.74044204e-02  1.93537414e-01\n",
      " -1.55786052e-01 -1.78729091e-02 -9.37459171e-01  2.44925067e-01\n",
      " -2.24154338e-01  3.24018866e-01 -2.07967877e-01  2.70428926e-01\n",
      " -1.03066944e-01 -1.54170349e-01  3.99111986e-01 -1.08410753e-01\n",
      " -2.40251020e-01  7.18544498e-02  3.10764432e-01  1.62550211e-02\n",
      " -1.18181534e-01  3.67666632e-01 -1.00532673e-01 -3.12455952e-01\n",
      " -2.55336557e-02  1.24032600e-02 -5.07644601e-02 -6.06864356e-02\n",
      " -1.34799704e-01  5.62716722e-01 -4.46999162e-01  9.99208819e-03\n",
      " -1.05011985e-02 -1.91234991e-01 -4.55346346e-01  8.39977041e-02\n",
      " -3.46922785e-01 -1.12798333e-01  2.49922752e-01 -3.24980050e-01\n",
      "  3.54002714e-02  3.92960817e-01 -1.91152766e-01 -2.81806797e-01\n",
      " -5.09225190e-01 -9.14767459e-02 -1.63784400e-01  1.53351024e-01\n",
      " -5.75623512e-01  2.04653263e-01  4.41294722e-02 -1.43252566e-01\n",
      " -5.07529080e-02  2.37555102e-01 -1.41447282e-03 -1.21625610e-01\n",
      " -3.89196545e-01 -5.61681800e-02  1.84735041e-02 -4.64849770e-02\n",
      "  2.84189079e-02  3.88638377e-02 -5.78378320e-01 -1.98933601e-01\n",
      " -6.54578388e-01 -2.08898321e-01  1.61051422e-01  6.86282218e-02\n",
      " -1.86340764e-01  3.37304287e-02 -2.95705885e-01  2.56646574e-02\n",
      " -3.18203062e-01 -3.38543244e-02  8.69825259e-02 -1.65467247e-01\n",
      " -3.07513356e-01  7.43200555e-02  3.53462905e-01 -2.66905040e-01\n",
      " -1.82728902e-01 -1.87545475e-02  2.89208684e-02  4.23821896e-01\n",
      "  1.02494217e-01  9.41635147e-02  1.88466489e-01  2.29482055e-01\n",
      "  1.72587726e-02 -3.72008950e-01 -1.63045362e-01  3.00885346e-02\n",
      " -2.09647462e-01  1.80983827e-01 -7.19894990e-02  9.07604620e-02\n",
      "  8.39722529e-02  1.13654613e-01 -7.66589642e-02  2.93442339e-01\n",
      "  3.08566213e-01 -1.55912682e-01  3.44167143e-01  3.77856612e-01\n",
      "  3.86568516e-01 -3.83628696e-01  1.31506518e-01 -1.73217371e-01\n",
      "  1.63670719e-01 -2.05111623e-01 -6.38212338e-02  1.16026260e-01\n",
      " -3.53874952e-01 -3.87074679e-01 -4.59273100e-01  2.34443009e-01\n",
      "  1.95995376e-01  3.65464121e-01 -2.23028973e-01 -4.20063883e-01\n",
      "  1.90422371e-01  3.26164722e-01  4.22025919e-01 -8.81390199e-02\n",
      " -1.61260232e-01 -4.01840992e-02  6.38023555e-01  4.32747990e-01\n",
      "  2.12451398e-01 -8.03059936e-02 -1.53411970e-01 -4.26394105e-01\n",
      " -5.43821370e-03  3.04960996e-01  2.18625963e-01 -2.14495242e-01\n",
      "  6.57047182e-02  2.10268095e-01  6.97246373e-01  7.70652667e-02\n",
      "  2.52830535e-02 -4.43788886e-01 -1.65758044e-01 -2.12652832e-02\n",
      "  2.09813580e-01  1.56542256e-01  5.21759331e-01  1.50104836e-01\n",
      " -1.86978921e-01  3.28466684e-01  3.09583515e-01  7.74470493e-02\n",
      " -5.27825832e-01  2.59915981e-02  5.24905086e-01  3.38634998e-01\n",
      " -1.33824900e-01 -8.89088511e-02 -5.39221577e-02 -6.89869702e-01\n",
      "  3.87615524e-02 -1.30642578e-01  1.48401842e-01  3.35592508e-01\n",
      "  7.05927610e-03  8.05654675e-02  4.37699445e-02 -6.18154824e-01\n",
      " -1.73192248e-01  9.72514153e-02  1.19350098e-01 -4.20425206e-01\n",
      "  7.29408205e-01 -3.67242247e-01 -9.37322080e-02  2.26034924e-01\n",
      " -4.15042676e-02 -1.46037057e-01  5.24536192e-01  8.73014554e-02\n",
      " -2.96411309e-02 -3.59353334e-01  4.04071994e-02 -1.07486896e-01\n",
      "  3.11743647e-01 -2.24270537e-01 -5.61840832e-01 -1.39995158e-01\n",
      "  8.14239203e-04 -2.89665520e-01  6.20135665e-02  2.02241540e-02\n",
      " -1.26386620e-02 -2.48153493e-01  2.38769487e-01  2.16282129e-01\n",
      "  4.88649821e-03 -4.52352762e-02  1.46446764e-01 -5.37910759e-01\n",
      " -2.98654437e-01  1.60891023e-02 -3.01470518e-01 -5.93786351e-02]\n"
     ]
    }
   ],
   "source": [
    "vector = ft_model[ft_model.key_to_index['работа']]\n",
    "print(vector)\n",
    "# w2v_model.save('fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "46e8312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(model, words):\n",
    "  all_words, mean = set(), []\n",
    "\n",
    "  for word in words:\n",
    "    if word in model.key_to_index:\n",
    "      mean.append(model[model.key_to_index[word]])\n",
    "      all_words.add(word)\n",
    "\n",
    "  if not mean:\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "  mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "  return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "fc84cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_list(model, text_list):\n",
    "  return np.vstack([word_averaging(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ea89fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = w2v_model[:]\n",
    "X = word_averaging_list(ft_model, X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "8a48d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b752",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "e5f43d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf61a48",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "57f92797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c6ff2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "fda8e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "93846bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "be4aad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "fba574ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.51      0.56     33421\n",
      "    positive       0.59      0.68      0.63     34396\n",
      "\n",
      "    accuracy                           0.60     67817\n",
      "   macro avg       0.60      0.60      0.59     67817\n",
      "weighted avg       0.60      0.60      0.60     67817\n",
      "\n",
      "f1 = 0.6313555312216951\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "46da7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a05d5e",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "fc2a0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0988b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "b8b15c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "9eddcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "81a6f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4b6099fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "2b167745",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac3b6c",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "06db6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "9bdced9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f5db7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "bdf6e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "27933407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "f9138857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "faab95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97447e04",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "992eb797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "aed2fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "184d343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "19987a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "d3354923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "73753458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.71      0.64     33421\n",
      "    positive       0.64      0.49      0.55     34396\n",
      "\n",
      "    accuracy                           0.60     67817\n",
      "   macro avg       0.61      0.60      0.59     67817\n",
      "weighted avg       0.61      0.60      0.59     67817\n",
      "\n",
      "f1 = 0.5507251159030538\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "28d80fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86293f6b",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "05c866ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "a7600ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "edd4dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "83bf1e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ff715022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "6c700e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.06      0.10     33421\n",
      "    positive       0.51      0.95      0.66     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.53      0.51      0.38     67817\n",
      "weighted avg       0.53      0.51      0.39     67817\n",
      "\n",
      "f1 = 0.6646534864160999\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "6385fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47d5c0",
   "metadata": {},
   "source": [
    "### Сеть c 2 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "fb349488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_2_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_2_layer = Net_2_layer().to(device)\n",
    "print(net_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "0ec234dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_2_layer = optim.SGD(net_2_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "588218ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "f186b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_2_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_2_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_2_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "eabd52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_2_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "f192166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n",
      "f1 = 0.6730259360355336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "print(f'f1 = {f1_score(y_test, ams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "0e4d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e1f48",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8e2bb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "f07b45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "8b7271dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "e7bff7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "5abd0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "bd5cd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.37      0.47     33421\n",
      "    positive       0.56      0.79      0.66     34396\n",
      "\n",
      "    accuracy                           0.58     67817\n",
      "   macro avg       0.60      0.58      0.56     67817\n",
      "weighted avg       0.60      0.58      0.56     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "489351c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7ed7e",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "0d16e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "ed72d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "125958a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "685374ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "8205f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "18dddb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "2599a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc8376",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "ffa07155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "b23ff992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "769d3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "2c4db27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "97bdd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "4e92dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "5e267c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ed8c4",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "c4ff136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a4d23372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "af4a5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "0e6fa886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "546b3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "8e6ee1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.69      0.63     33421\n",
      "    positive       0.63      0.53      0.58     34396\n",
      "\n",
      "    accuracy                           0.61     67817\n",
      "   macro avg       0.61      0.61      0.60     67817\n",
      "weighted avg       0.61      0.61      0.60     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "54ce3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd4d8a",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "1a82c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "96f0dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "0e70f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "1664b5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "efacb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "356f48e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "94cddf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bed3dd",
   "metadata": {},
   "source": [
    "### Сеть c 3 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "dbbe3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_3_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_3_layer = Net_3_layer().to(device)\n",
    "print(net_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "33ea3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_3_layer = optim.SGD(net_3_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "5e70f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "ae004626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_3_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_3_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_3_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "80fd701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_3_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "2a84b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "06525ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7a6e1",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "5ef1a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "d61c60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "de124a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "0490832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "ebfe5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "d37aa88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "8efa0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7e71e",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e4637b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "2988a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "62c3ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "d3f1e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "b126e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "55016ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "67a4efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2601a1d",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, разные слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "6c1c223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.fc6 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "5bbb378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "43adf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "4a1676d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "a39c1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "53e9d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "dcecebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b817c6",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "bc45ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "f61a94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.01) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "f98d16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "f0e99b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "89128db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "cfbbadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.67      0.63     33421\n",
      "    positive       0.63      0.54      0.58     34396\n",
      "\n",
      "    accuracy                           0.60     67817\n",
      "   macro avg       0.61      0.60      0.60     67817\n",
      "weighted avg       0.61      0.60      0.60     67817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "36964847",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e779bb",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "b9ddc22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "14bd9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "c07496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "22aa9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "c17cebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "18f9bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "ff906d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede651f",
   "metadata": {},
   "source": [
    "### Сеть c 4 скрытыми слоями, одинаковые слои, lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "e765512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_4_layer(\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура нейронной сети\n",
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net_4_layer = Net_4_layer().to(device)\n",
    "print(net_4_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "6a2e0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задаем оптимизатор и функцию потерь\n",
    "\n",
    "\n",
    "optimizer_4_layer = optim.SGD(net_4_layer.parameters(), lr=0.0001) # регулирует изменяемые параметры нашей модели, а именно веса, чтобы они постепенно, с течением времени, приходили в соответствие нашим данным\n",
    "# lr - скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз\n",
    "# Каждый полный проход по данным называется эпохой (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "0e010309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование в тензоры\n",
    "inputs_train = torch.tensor(X_train).to(device)\n",
    "targets_train = torch.tensor(y_train).int().to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test).to(device)\n",
    "targets_test = torch.tensor(y_test).int().to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=32)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "5629d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): # 15 полных прохода по нашим данным\n",
    "    for data in trainset:  # `data` это батч наших данных\n",
    "        X, y = data[0].to(device), data[1].to(device)  # X это батч свойств, y это батч целевых переменных.\n",
    "        net_4_layer.zero_grad()  # устанавливаем значение градиента в 0 перед вычислением функции потерь. Вам следует делать это на каждом шаге.\n",
    "        output = net_4_layer(X.float())  # передаем батч\n",
    "        #loss = F.nll_loss(output, y.long())  # вычисляем функцию потерь\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        loss.backward()  # передаем это значение назад по сети\n",
    "        optimizer_4_layer.step()  # пытаемся оптимизировать значение весов исходя из потерь и градиента\n",
    "    print(loss)  # выводим на экран значение функции потерь. Мы надеемся, что оно убывает!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "a06f123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предсказание меток классов для примеров из тестового множества\n",
    "ams = []\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        output = net_4_layer(X.float())\n",
    "        for idx, i in enumerate(output):\n",
    "            ams.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "74ba458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "67817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     33421\n",
      "    positive       0.51      1.00      0.67     34396\n",
      "\n",
      "    accuracy                           0.51     67817\n",
      "   macro avg       0.25      0.50      0.34     67817\n",
      "weighted avg       0.26      0.51      0.34     67817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ext-shirokov-m@ad.speechpro.com/shirokov/jupyter/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Оценка качества классификации\n",
    "print(output.size())\n",
    "print(len(ams))\n",
    "\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(y_test, ams, target_names=target_names))\n",
    "# print(f'f1 = {f1_score(y_test, ams, average='weighted', labels = [0, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "0531f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ft_pre_trained.append(f1_score(y_test, ams, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "98b24537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-March-2023 11:11:22\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c97b",
   "metadata": {},
   "source": [
    "# Сводная таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "f381268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Количество скрытых слоев</th>\n",
       "      <th>Количество нейронов</th>\n",
       "      <th>Скорость обучения</th>\n",
       "      <th>Weighted F1-score self-trained W2V</th>\n",
       "      <th>Weighted F1-score self-trained FT</th>\n",
       "      <th>Weighted F1-score pre-trained W2V</th>\n",
       "      <th>Weighted F1-score pre-trained FT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Количество скрытых слоев Количество нейронов Скорость обучения  \\\n",
       "0                          2                16-8              0.01   \n",
       "1                          2                16-8             0.001   \n",
       "2                          2                16-8            0.0001   \n",
       "3                          2               16-16              0.01   \n",
       "4                          2               16-16             0.001   \n",
       "5                          2               16-16            0.0001   \n",
       "6                          3             32-16-8              0.01   \n",
       "7                          3             32-16-8             0.001   \n",
       "8                          3             32-16-8            0.0001   \n",
       "9                          3            32-32-32              0.01   \n",
       "10                         3            32-32-32             0.001   \n",
       "11                         3            32-32-32            0.0001   \n",
       "12                         4          64-32-16-8              0.01   \n",
       "13                         4          64-32-16-8             0.001   \n",
       "14                         4          64-32-16-8            0.0001   \n",
       "15                         4         64-64-64-64              0.01   \n",
       "16                         4         64-64-64-64             0.001   \n",
       "17                         4         64-64-64-64            0.0001   \n",
       "\n",
       "    Weighted F1-score self-trained W2V  Weighted F1-score self-trained FT  \\\n",
       "0                                 0.66                               0.68   \n",
       "1                                 0.34                               0.66   \n",
       "2                                 0.34                               0.34   \n",
       "3                                 0.67                               0.68   \n",
       "4                                 0.66                               0.67   \n",
       "5                                 0.34                               0.34   \n",
       "6                                 0.67                               0.68   \n",
       "7                                 0.66                               0.34   \n",
       "8                                 0.34                               0.34   \n",
       "9                                 0.67                               0.68   \n",
       "10                                0.66                               0.66   \n",
       "11                                0.34                               0.34   \n",
       "12                                0.66                               0.67   \n",
       "13                                0.66                               0.34   \n",
       "14                                0.34                               0.34   \n",
       "15                                0.66                               0.67   \n",
       "16                                0.66                               0.34   \n",
       "17                                0.34                               0.34   \n",
       "\n",
       "    Weighted F1-score pre-trained W2V  Weighted F1-score pre-trained FT  \n",
       "0                                0.59                              0.60  \n",
       "1                                0.34                              0.34  \n",
       "2                                0.34                              0.34  \n",
       "3                                0.59                              0.59  \n",
       "4                                0.58                              0.39  \n",
       "5                                0.34                              0.34  \n",
       "6                                0.61                              0.56  \n",
       "7                                0.59                              0.34  \n",
       "8                                0.34                              0.34  \n",
       "9                                0.60                              0.60  \n",
       "10                               0.59                              0.34  \n",
       "11                               0.34                              0.34  \n",
       "12                               0.61                              0.34  \n",
       "13                               0.34                              0.34  \n",
       "14                               0.34                              0.34  \n",
       "15                               0.60                              0.60  \n",
       "16                               0.34                              0.34  \n",
       "17                               0.34                              0.34  "
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame()\n",
    "summary['Количество скрытых слоев'] = np.array([2] * 6 + [3] * 6 + [4] * 6, dtype=int)\n",
    "summary['Количество нейронов'] = np.array(['16-8'] * 3 + ['16-16'] * 3 + ['32-16-8'] * 3 + ['32-32-32'] * 3 + ['64-32-16-8'] * 3 + ['64-64-64-64'] * 3, dtype=str)\n",
    "summary['Скорость обучения'] = np.array(['0.01', '0.001', '0.0001'] * 6, dtype=str)\n",
    "summary['Weighted F1-score self-trained W2V'] = np.array([round(el, 2) for el in f1_score_w2v_self_trained], dtype=float)\n",
    "summary['Weighted F1-score self-trained FT'] = np.array([round(el, 2) for el in f1_score_ft_self_trained], dtype=float)\n",
    "summary['Weighted F1-score pre-trained W2V'] = np.array([round(el, 2) for el in f1_score_w2v_pre_trained], dtype=float)\n",
    "summary['Weighted F1-score pre-trained FT'] = np.array([round(el, 2) for el in f1_score_ft_pre_trained], dtype=float)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3dec8",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1668b5a",
   "metadata": {},
   "source": [
    "При использовании предобученных моделей для признаков результаты нейронных сетей получились хуже, скорее всего это связано с тем, что обучение шло на полных словах, без опечаток, а представленный набор данных содержит некоторое количество твитов, где часто встречаются дефектные слова. Но последнее улучшит результат и для самостоятельно обученных моделей признаков. В сводной таблице можно заметить увеличение количество застревания метода градиентного спуска в локальных минимумах, особенно при увеличении количества скрытых слоев."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P75gWMshhfv_",
    "_Q6YfGFAgyW-",
    "pZTCtGBdmPCN",
    "WKGUJrpWmbNX",
    "jcrq3YYfmdth",
    "Tv9-ok4Qmhb6",
    "JRiPVfwVOOdY",
    "MSvRndCKo9nM",
    "lSadA7atpSP2",
    "YX47aVQ7pbVj",
    "Sz1ERkCEpetp",
    "kYbqQt_zpgjL",
    "B5C1IynVf7Ry"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
