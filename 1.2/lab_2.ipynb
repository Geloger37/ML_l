{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "P75gWMshhfv_",
   "metadata": {
    "id": "P75gWMshhfv_"
   },
   "source": [
    "# Начальная инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7bda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 07:56:07\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65078f9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65078f9a",
    "outputId": "0b40f77d-a829-4f88-8e9d-83f02bf0e4b5"
   },
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "# !pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "# !pip install scikit-learn\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch.optim as optim\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import re\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from string import punctuation\n",
    "\n",
    "# %pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qZMoTvwu7wGy",
   "metadata": {
    "id": "qZMoTvwu7wGy"
   },
   "outputs": [],
   "source": [
    "# Подключение вычислений на видеокарту, если доступна\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "GnUj7tRT96cu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnUj7tRT96cu",
    "outputId": "c0f3c9ff-9d0e-4157-e914-4d1220d0e00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2ff9c",
   "metadata": {},
   "source": [
    "# Модели и методы для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560415c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc4 = nn.Linear(layers[2], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f951018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_3_layer(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc4 = nn.Linear(layers[2], layers[3])\n",
    "        self.fc5 = nn.Linear(layers[3], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5155802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_4_layer(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()  #вх. #вых.\n",
    "        self.fc1 = nn.Linear(300, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc4 = nn.Linear(layers[2], layers[3])\n",
    "        self.fc5 = nn.Linear(layers[3], layers[4])\n",
    "        self.fc6 = nn.Linear(layers[4], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0319f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net = None, learning_rate = None, x = None, y = None, batch = None, epochs = None, device = None):\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    inputs_train = torch.tensor(x).to(device)\n",
    "    targets_train = torch.tensor(y).int().to(device)\n",
    "\n",
    "    train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "\n",
    "    trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batch)\n",
    "\n",
    "    print('-'*30)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = None\n",
    "        for data in trainset:\n",
    "            X, Y = data[0].to(device), data[1].to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X.float())\n",
    "            loss = F.cross_entropy(output, Y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "    \n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe635a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(net = None, device = None, x = None, y = None, batch = None):\n",
    "    inputs_test = torch.tensor(x).to(device)\n",
    "    targets_test = torch.tensor(y).int().to(device)\n",
    "    \n",
    "    test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "    testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=batch)\n",
    "\n",
    "    ams = []\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, Y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            output = net(X.float())\n",
    "            for idx, i in enumerate(output):\n",
    "                ams.append(torch.argmax(i).item())\n",
    "    return f1_score(y, ams, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803c9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(model, words):\n",
    "    all_words, mean = set(), []\n",
    "\n",
    "    for word in words:\n",
    "        mean.append(model.wv[word])\n",
    "        all_words.add(word)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "def word_averaging_list(model, text_list):\n",
    "    return np.vstack([word_averaging(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03a7ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_pre_trained(model, words):\n",
    "  all_words, mean = set(), []\n",
    "\n",
    "  for word in words:\n",
    "    if word in model.key_to_index:\n",
    "      mean.append(model[model.key_to_index[word]])\n",
    "      all_words.add(word)\n",
    "\n",
    "  if not mean:\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "  mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "  return mean\n",
    "\n",
    "\n",
    "def word_averaging_list_pre_trained(model, text_list):\n",
    "  return np.vstack([word_averaging_pre_trained(model, comment_text) for comment_text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0653fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 07:56:12\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a07c13",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd654ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('X_y_train.csv', sep=';')\n",
    "test = pd.read_csv('X_y_test.csv', sep=';')\n",
    "\n",
    "X_train = [el[0] for el in train[['Text']].values]\n",
    "X_test = [el[0] for el in test[['Text']].values]\n",
    "y_train = [el[0] for el in train[['Class']].replace(-1, 0).values]\n",
    "y_test = [el[0] for el in test[['Class']].replace(-1, 0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc82b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a X_token file one time!\n",
    "\n",
    "# def tokenize(text):\n",
    "\n",
    "#   text_token = nltk.word_tokenize(text)\n",
    "#   text_word = [el.lower() for el in text_token if el not in punctuation]\n",
    "#   return text_word\n",
    "\n",
    "# X_train_token = [tokenize(t) for t in X_train]\n",
    "# X_test_token = [tokenize(t) for t in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfbb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_train_token.txt', mode='w+') as file:\n",
    "#     for sentence in X_train_token:\n",
    "#         print(*sentence, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ec4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token = [sentence.split() for sentence in open('X_token.txt', mode='r')]\n",
    "X_train_token = [sentence.split() for sentence in open('X_train_token.txt', mode='r')]\n",
    "X_test_token = [sentence.split() for sentence in open('X_test_token.txt', mode='r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50774f03",
   "metadata": {},
   "source": [
    "Функции для работы предобученных моделей RusVectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b202a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# !pip install wget\n",
    "import wget\n",
    "import re\n",
    "# !pip install ufal.udpipe\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_ADJ трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = \"x\" * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(\" \", \"\")\n",
    "    if token == \"Файл\" and \"SpaceAfter=No\" in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "    if \"|\" in out_lemma or out_lemma.endswith(\".jpg\") or out_lemma.endswith(\".png\"):\n",
    "        return None\n",
    "    if pos != \"PUNCT\":\n",
    "        if out_lemma.startswith(\"«\") or out_lemma.startswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[1:])\n",
    "        if out_lemma.endswith(\"«\") or out_lemma.endswith(\"»\"):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "        if (\n",
    "            out_lemma.endswith(\"!\")\n",
    "            or out_lemma.endswith(\"?\")\n",
    "            or out_lemma.endswith(\",\")\n",
    "            or out_lemma.endswith(\".\")\n",
    "        ):\n",
    "            out_lemma = \"\".join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace(\n",
    "        \"\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019\",\n",
    "        \"\\u0022\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF\", \"\\u2003\\u002D\\u002D\\u2003\", text\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2010\\u2011\", \"\\u002D\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000\",\n",
    "        \"\\u2002\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = re.sub(\"\\u2003\\u2003\", \"\\u2003\", text)\n",
    "    text = re.sub(\"\\t\\t\", \"\\t\", text)\n",
    "\n",
    "    text = list_replace(\n",
    "        \"\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062\",\n",
    "        \".\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = list_replace(\"\\u2217\", \"\\u002A\", text)\n",
    "\n",
    "    text = list_replace(\"…\", \"...\", text)\n",
    "\n",
    "    text = list_replace(\"\\u2241\\u224B\\u2E2F\\u0483\", \"\\u223D\", text)\n",
    "\n",
    "    text = list_replace(\"\\u00C4\", \"A\", text)  # латинская\n",
    "    text = list_replace(\"\\u00E4\", \"a\", text)\n",
    "    text = list_replace(\"\\u00CB\", \"E\", text)\n",
    "    text = list_replace(\"\\u00EB\", \"e\", text)\n",
    "    text = list_replace(\"\\u1E26\", \"H\", text)\n",
    "    text = list_replace(\"\\u1E27\", \"h\", text)\n",
    "    text = list_replace(\"\\u00CF\", \"I\", text)\n",
    "    text = list_replace(\"\\u00EF\", \"i\", text)\n",
    "    text = list_replace(\"\\u00D6\", \"O\", text)\n",
    "    text = list_replace(\"\\u00F6\", \"o\", text)\n",
    "    text = list_replace(\"\\u00DC\", \"U\", text)\n",
    "    text = list_replace(\"\\u00FC\", \"u\", text)\n",
    "    text = list_replace(\"\\u0178\", \"Y\", text)\n",
    "    text = list_replace(\"\\u00FF\", \"y\", text)\n",
    "    text = list_replace(\"\\u00DF\", \"s\", text)\n",
    "    text = list_replace(\"\\u1E9E\", \"S\", text)\n",
    "\n",
    "    currencies = list(\n",
    "        \"\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192\"\n",
    "    )\n",
    "\n",
    "    alphabet = list(\n",
    "        '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
    "    )\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = \"\".join(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def process(pipeline, text=\"Строка\", keep_pos=True, keep_punct=False):\n",
    "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "    entities = {\"PROPN\"}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [line for line in processed.split(\"\\n\") if not line.startswith(\"#\")]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split(\"\\t\") for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if \"|\" not in feats:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split(\"=\")[0]: el.split(\"=\")[1] for el in feats.split(\"|\")}\n",
    "            if \"Case\" not in morph or \"Number\" not in morph:\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph[\"Case\"]\n",
    "                mem_number = morph[\"Number\"]\n",
    "            if morph[\"Case\"] == mem_case and morph[\"Number\"] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if \"SpacesAfter=\\\\n\" in misc or \"SpacesAfter=\\s\\\\n\" in misc:\n",
    "                    named = False\n",
    "                    past_lemma = \"::\".join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if (\n",
    "                    pos == \"NUM\" and token.isdigit()\n",
    "                ):  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = \"::\".join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + \"_PROPN\")\n",
    "                tagged_propn.append(\"%s_%s\" % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split(\"_\")[1] != \"PUNCT\"]\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split(\"_\")[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = \"https://rusvectores.org/static/models/udpipe_syntagrus.model\"\n",
    "udpipe_filename = '' + udpipe_model_url.split(\"/\")[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print(\"UDPipe model not found. Downloading...\", file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print(\"\\nLoading the model...\", file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(\n",
    "    model, \"tokenize\", Pipeline.DEFAULT, Pipeline.DEFAULT, \"conllu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e5f0fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 07:56:16\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IVxnHGLRdEAF",
   "metadata": {
    "id": "IVxnHGLRdEAF"
   },
   "source": [
    "# Word2vec-признаки, обученные самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d43f1",
   "metadata": {},
   "source": [
    "## Обучение модели word2vec на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b4db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=X_token, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deeed725",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a500772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78028021, 92066100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(X_token, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n",
    "# w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = w2v_model.wv['работа']\n",
    "# print(vector)\n",
    "w2v_model.save('self-trained_word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc155ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "X_train = word_averaging_list(w2v_model, X_train_token)\n",
    "X_test = word_averaging_list(w2v_model, X_test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "011b553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 07:57:24\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacea138",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5631fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f1_score_w2v_self_trained = []\n",
    "\n",
    "for layers in [[16, 8, 4], [16, 16, 16], [32, 16, 8, 4], [32, 32, 32, 32], [64, 32, 16, 8, 4], [64, 64, 64, 64, 64]]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:\n",
    "# for layers in [[16, 8, 4]]:\n",
    "    # for lr in [0.01]:    \n",
    "        net = None\n",
    "        if len(layers) - 1 == 2:\n",
    "            net = Net_2_layer(layers=layers)\n",
    "        elif len(layers) - 1 == 3:\n",
    "            net = Net_3_layer(layers=layers)\n",
    "        else:\n",
    "            net = Net_4_layer(layers=layers)\n",
    "\n",
    "        net = train_net(net=net, learning_rate=lr, x=X_train, y=y_train, batch=1, epochs=EPOCHS, device=device)\n",
    "        f1_score_w2v_self_trained.append(test_net(net=net, device=device, x=X_test, y=y_test, batch=1))\n",
    "\n",
    "        # f1_score_w2v_self_trained.append(train_net(net=net, learning_rate=lr, x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_test, batch=1, epochs=10, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4a23a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 09:08:22\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207a99e",
   "metadata": {},
   "source": [
    "# FastText-признаки, обученные самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ac745",
   "metadata": {},
   "source": [
    "## Обучение модели FastText на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "693d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sentences=X_token, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a5c97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.build_vocab(X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bad328dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78021401, 92066100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.train(X_token, total_examples=ft_model.corpus_count, epochs=300, report_delay=1)\n",
    "# ft_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9e6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ft_model.wv['работа']\n",
    "# print(vector)\n",
    "ft_model.save('self-trained_fasttext/fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ef21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = ft_model.wv\n",
    "X_train = word_averaging_list(ft_model, X_train_token)\n",
    "X_test = word_averaging_list(ft_model, X_test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42f3ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 09:13:34\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b5a5e",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de7dde48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f1_score_ft_self_trained = []\n",
    "\n",
    "for layers in [[16, 8, 4], [16, 16, 16], [32, 16, 8, 4], [32, 32, 32, 32], [64, 32, 16, 8, 4], [64, 64, 64, 64, 64]]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:\n",
    "# for layers in [[16, 8, 4]]:\n",
    "    # for lr in [0.01]:    \n",
    "        net = None\n",
    "        if len(layers) - 1 == 2:\n",
    "            net = Net_2_layer(layers=layers)\n",
    "        elif len(layers) - 1 == 3:\n",
    "            net = Net_3_layer(layers=layers)\n",
    "        else:\n",
    "            net = Net_4_layer(layers=layers)\n",
    "\n",
    "        net = train_net(net=net, learning_rate=lr, x=X_train, y=y_train, batch=1, epochs=EPOCHS, device=device)\n",
    "        f1_score_ft_self_trained.append(test_net(net=net, device=device, x=X_test, y=y_test, batch=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6944ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 10:24:32\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e77b08",
   "metadata": {},
   "source": [
    "# Word2Vec-признаки предобученные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098e93e",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe22f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('X_y_train.csv', sep=';')\n",
    "test = pd.read_csv('X_y_test.csv', sep=';')\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "X_train_token_pre_trained = []\n",
    "for input_line in train['Text']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res)\n",
    "    X_train_token_pre_trained.append(output)\n",
    "\n",
    "\n",
    "X_test_token_pre_trained = []\n",
    "for input_line in test['Text']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res)\n",
    "    X_test_token_pre_trained.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31e53260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 10:26:15\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac7ac6",
   "metadata": {},
   "source": [
    "## Обучение модели Word2Vec на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70cfe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec/model.bin', encoding='utf-8', unicode_errors='ignore', binary=True)\n",
    "w2v_model.fill_norms(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d4acd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.02568102e+00  4.10510159e+00  3.96478510e+00  1.99146974e+00\n",
      "  2.75750041e-01  1.05907798e+00 -2.34842032e-01  1.13069057e+00\n",
      " -3.34521389e+00  5.44947767e+00 -1.78760219e+00  2.93239641e+00\n",
      "  4.24604845e+00 -3.08241105e+00 -3.06380081e+00  4.14739418e+00\n",
      "  1.94640350e+00 -6.41723156e+00  4.48100597e-01  1.98949501e-01\n",
      " -1.96533740e+00  2.04884505e+00  6.81714356e-01  1.89991868e+00\n",
      " -1.87503803e+00  1.61289966e+00  8.80351245e-01  1.27756655e+00\n",
      " -1.60905108e-01 -2.56419826e+00  5.59642196e-01 -2.38538122e+00\n",
      "  5.61529458e-01 -3.78697932e-01 -5.17279339e+00 -4.75222081e-01\n",
      " -7.38774776e-01 -1.47579443e+00  5.36987162e+00  1.66592920e+00\n",
      "  2.48067904e+00 -3.41140532e+00 -3.26146185e-01  1.99449539e+00\n",
      "  1.23095262e+00  1.58154178e+00  1.74632573e+00  1.36116230e+00\n",
      " -2.66357565e+00  6.55228496e-01  2.66324496e+00 -2.19835329e+00\n",
      " -7.47147948e-02  2.37323642e+00 -2.46333432e+00  2.09079415e-01\n",
      " -2.15091515e+00 -5.49663115e+00 -8.21452737e-01  1.57285839e-01\n",
      "  6.61701798e-01 -3.56726438e-01 -3.01842904e+00  7.32664943e-01\n",
      "  2.08598614e+00 -1.41081357e+00 -5.02162099e-01 -3.44310713e+00\n",
      " -2.70832729e+00 -1.67359900e+00 -4.65843487e+00  4.59569216e+00\n",
      " -3.65738130e+00 -1.68344200e+00  1.12880623e+00 -2.32587099e+00\n",
      "  2.25881290e+00 -5.71228564e-01 -1.42057741e+00 -9.63704064e-02\n",
      " -7.41252661e-01  2.72428006e-01  5.86827815e-01 -1.91268504e+00\n",
      " -1.54868975e-01  5.33265948e-01 -9.93554652e-01  1.19762135e+00\n",
      "  1.97423756e+00 -2.44773769e+00 -4.29723382e-01 -2.19450855e+00\n",
      " -8.78581703e-01 -3.68831038e-01 -4.73884058e+00 -9.95291829e-01\n",
      "  1.96129644e+00 -2.23322058e+00 -1.35348892e+00  1.84871805e+00\n",
      "  2.98968244e+00  4.03689671e+00 -1.15203686e-01  1.97898507e+00\n",
      "  2.20201778e+00  2.63293362e+00 -1.79020613e-01  1.63138545e+00\n",
      " -7.25692570e-01  2.64699012e-01 -1.40380383e+00  1.80271244e+00\n",
      " -5.17523718e+00  4.29810143e+00 -1.96248770e+00 -1.80293024e-01\n",
      "  4.80625749e-01 -2.52803373e+00  9.15996060e-02 -1.33048618e+00\n",
      " -9.11542594e-01  3.21305346e+00  2.29246783e+00  1.28174770e+00\n",
      "  9.81810153e-01 -3.35683489e+00  3.94806170e+00  2.68293023e-01\n",
      " -4.58949536e-01 -1.18627512e+00  6.55168951e-01 -6.76943883e-02\n",
      " -2.29546592e-01 -2.00870585e+00  3.66676283e+00  1.87209797e+00\n",
      " -1.45867634e+00 -2.36221835e-01  3.55628419e+00 -1.16016120e-01\n",
      "  9.67946708e-01  7.77125597e-01  1.11526275e+00 -2.28089309e+00\n",
      "  8.53556693e-01  1.52941298e+00 -7.59864867e-01 -2.07443190e+00\n",
      "  1.91821420e+00  2.16583580e-01 -3.45948243e+00  2.75355172e+00\n",
      " -3.14419675e+00  3.06619215e+00  2.17322350e+00 -8.57929528e-01\n",
      "  3.04637694e+00  2.34819007e+00  1.03708768e+00 -2.49978733e+00\n",
      " -3.83608341e+00 -3.33338594e+00  3.34870982e+00 -1.91703122e-02\n",
      "  1.28278577e+00 -2.13554525e+00  8.76425862e-01  1.94094157e+00\n",
      "  1.48071671e+00 -8.53495061e-01 -8.66016686e-01  3.01546168e+00\n",
      "  4.24282014e-01 -5.61698258e-01 -1.72610307e+00  4.98404074e+00\n",
      " -2.34956574e+00  7.54057050e-01  6.40150642e+00 -4.51727629e+00\n",
      "  7.40165651e-01 -8.57133865e-01 -3.01201391e+00 -2.03801298e+00\n",
      " -4.95920330e-01 -4.75771904e+00  1.89506114e+00  2.62465096e+00\n",
      " -1.93081462e+00 -3.10070252e+00 -5.64702928e-01  3.94759560e+00\n",
      " -6.75520897e-01 -5.14421701e-01 -2.28597283e+00 -5.10558891e+00\n",
      " -1.69497812e+00 -4.99807626e-01 -4.22593355e+00 -3.29628015e+00\n",
      " -3.64795351e+00  4.27742064e-01  1.55653608e+00 -1.20746028e+00\n",
      "  1.16968453e+00 -4.12662745e+00  6.82037175e-01 -6.08240008e-01\n",
      "  3.56517601e+00 -2.82430887e+00 -2.33061457e+00  1.25689173e+00\n",
      "  2.00057721e+00 -1.98435462e+00  2.28040171e+00 -4.27699947e+00\n",
      " -7.83862472e-01 -6.97373772e+00 -3.73213589e-01 -2.76460260e-01\n",
      "  1.53282499e+00 -5.59267163e-01 -2.47404599e+00 -4.66384125e+00\n",
      "  1.36715043e+00  1.13244104e+00  1.92361784e+00 -3.44760823e+00\n",
      " -1.85811722e+00 -5.99661022e-02 -1.47647813e-01  3.33556509e+00\n",
      " -1.07680261e+00 -2.74878240e+00 -1.08012426e+00  4.64338350e+00\n",
      " -1.33808827e+00  1.01821266e-01  1.19078994e+00 -3.83346200e+00\n",
      " -2.95207191e+00  4.63828325e-01  3.74571919e-01 -1.87971961e+00\n",
      "  4.65013683e-01 -2.98640633e+00  1.85878265e+00  1.87133837e+00\n",
      " -1.41013324e+00 -1.76775730e+00 -1.67912006e+00  6.38692498e-01\n",
      "  6.00675154e+00  1.78430486e+00  5.12833929e+00  1.16797268e+00\n",
      "  7.20571876e-01 -7.98638880e-01 -1.85424268e+00 -3.97217155e-01\n",
      "  2.13334227e+00  1.77917492e+00  2.41807723e+00  2.63354468e+00\n",
      "  8.88301134e-02  1.47717535e+00  4.51532936e+00  2.33813906e+00\n",
      " -1.32290244e+00 -1.43293786e+00  2.42255464e-01  2.47964597e+00\n",
      "  9.54133809e-01 -3.64849389e-01  4.55634773e-01 -1.37747264e+00\n",
      " -2.67389506e-01 -3.57467103e+00  1.96309328e+00  1.32332194e+00\n",
      "  2.98427701e+00  4.46703047e-01 -1.31625247e+00 -7.52604663e-01\n",
      "  1.47587705e+00 -5.63624322e-01 -3.61198211e+00  5.86269081e-01\n",
      "  3.10935116e+00 -3.36393505e-01  1.92764625e-01  3.11378855e-03\n",
      " -9.74766254e-01  9.20407593e-01  1.70369864e+00  1.72775781e+00\n",
      "  1.49772477e+00 -5.23070872e-01  1.70956588e+00  1.36436200e+00]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_model[w2v_model.key_to_index['работа_NOUN']]\n",
    "print(vector)\n",
    "# w2v_model.save('fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5822ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = w2v_model[:]\n",
    "X_train = word_averaging_list_pre_trained(w2v_model, X_train_token_pre_trained)\n",
    "X_test = word_averaging_list_pre_trained(w2v_model, X_test_token_pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f30ee483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 10:26:20\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b4db6",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1822139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f1_score_w2v_pre_trained = []\n",
    "\n",
    "for layers in [[16, 8, 4], [16, 16, 16], [32, 16, 8, 4], [32, 32, 32, 32], [64, 32, 16, 8, 4], [64, 64, 64, 64, 64]]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:\n",
    "# for layers in [[16, 8, 4]]:\n",
    "    # for lr in [0.01]:    \n",
    "        net = None\n",
    "        if len(layers) - 1 == 2:\n",
    "            net = Net_2_layer(layers=layers)\n",
    "        elif len(layers) - 1 == 3:\n",
    "            net = Net_3_layer(layers=layers)\n",
    "        else:\n",
    "            net = Net_4_layer(layers=layers)\n",
    "\n",
    "        net = train_net(net=net, learning_rate=lr, x=X_train, y=y_train, batch=1, epochs=EPOCHS, device=device)\n",
    "        f1_score_w2v_pre_trained.append(test_net(net=net, device=device, x=X_test, y=y_test, batch=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44c02664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 11:38:27\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52ff52",
   "metadata": {},
   "source": [
    "# FastText-признаки предобученные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3d699",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70d470a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('X_y_train.csv', sep=';')\n",
    "test = pd.read_csv('X_y_test.csv', sep=';')\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "X_train_token_pre_trained = []\n",
    "for input_line in train['Text']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res, keep_pos=False)\n",
    "    X_train_token_pre_trained.append(output)\n",
    "\n",
    "\n",
    "X_test_token_pre_trained = []\n",
    "for input_line in test['Text']:\n",
    "    res = unify_sym(input_line.strip())\n",
    "    output = process(process_pipeline, text=res, keep_pos=False)\n",
    "    X_test_token_pre_trained.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c313da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 13:29:04\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f798ed",
   "metadata": {},
   "source": [
    "## Обучение модели FastText на полученном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b1451cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = gensim.models.KeyedVectors.load('fasttext/model.model')\n",
    "ft_model.fill_norms(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48c132b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.28143653e-01  6.31993353e-01  3.97833914e-01 -1.69036105e-01\n",
      "  1.94467843e-01  2.62616009e-01 -1.29946560e-01  2.99574584e-01\n",
      "  5.06506674e-02  5.92105603e-03  8.87992755e-02  4.69435602e-01\n",
      " -2.00550243e-01  6.64121136e-02  1.52241021e-01 -3.72883081e-01\n",
      " -3.08863670e-02 -1.04671396e-01  1.48081467e-01 -8.00064430e-02\n",
      "  7.97154009e-02 -1.13625549e-01 -3.49553585e-01 -1.42057359e-01\n",
      "  7.67205238e-01 -1.68227687e-01 -1.12291731e-01  3.17529649e-01\n",
      " -3.59128028e-01 -6.41788542e-02 -5.57220131e-02  1.65108591e-01\n",
      "  1.66394070e-01  5.43315411e-01 -1.59994856e-01  2.17812255e-01\n",
      "  2.80040950e-01 -5.70807010e-02  1.74972832e-01  1.98939666e-01\n",
      "  1.93150759e-01  2.96664566e-01  7.72891268e-02  5.42501509e-01\n",
      "  2.76810322e-02 -1.13098420e-01  1.08400442e-01  6.68987632e-03\n",
      " -1.36405602e-01 -9.27114263e-02 -5.01722358e-02 -3.12990457e-01\n",
      "  7.97539577e-02  3.24938953e-01 -1.21801049e-01  2.70672590e-01\n",
      " -4.02754359e-02  3.37236971e-01 -4.76787239e-01 -1.47199020e-01\n",
      "  8.80351439e-02 -2.75021195e-01 -6.54799163e-01 -8.04591656e-01\n",
      "  4.31546926e-01  2.73890465e-01 -1.09550454e-01  6.12769544e-01\n",
      " -2.65803132e-02  3.61043900e-01  3.85290504e-01  1.24445118e-01\n",
      " -3.23237568e-01  1.86322227e-01 -1.48882821e-01  2.89316565e-01\n",
      " -4.88742322e-01 -3.43459874e-01  9.79158878e-02  5.26423519e-03\n",
      "  7.48121142e-02 -1.55417221e-02 -1.04115635e-01 -1.79251507e-01\n",
      "  3.09590220e-01  1.14156574e-01  1.82896614e-01 -5.79745471e-01\n",
      " -3.30249161e-01  1.98455855e-01 -2.51652986e-01 -2.51863408e-03\n",
      "  4.93122250e-01  2.09248379e-01 -1.62874639e-01  2.51050502e-01\n",
      "  3.56166512e-01  6.87424466e-02 -9.74044204e-02  1.93537414e-01\n",
      " -1.55786052e-01 -1.78729091e-02 -9.37459171e-01  2.44925067e-01\n",
      " -2.24154338e-01  3.24018866e-01 -2.07967877e-01  2.70428926e-01\n",
      " -1.03066944e-01 -1.54170349e-01  3.99111986e-01 -1.08410753e-01\n",
      " -2.40251020e-01  7.18544498e-02  3.10764432e-01  1.62550211e-02\n",
      " -1.18181534e-01  3.67666632e-01 -1.00532673e-01 -3.12455952e-01\n",
      " -2.55336557e-02  1.24032600e-02 -5.07644601e-02 -6.06864356e-02\n",
      " -1.34799704e-01  5.62716722e-01 -4.46999162e-01  9.99208819e-03\n",
      " -1.05011985e-02 -1.91234991e-01 -4.55346346e-01  8.39977041e-02\n",
      " -3.46922785e-01 -1.12798333e-01  2.49922752e-01 -3.24980050e-01\n",
      "  3.54002714e-02  3.92960817e-01 -1.91152766e-01 -2.81806797e-01\n",
      " -5.09225190e-01 -9.14767459e-02 -1.63784400e-01  1.53351024e-01\n",
      " -5.75623512e-01  2.04653263e-01  4.41294722e-02 -1.43252566e-01\n",
      " -5.07529080e-02  2.37555102e-01 -1.41447282e-03 -1.21625610e-01\n",
      " -3.89196545e-01 -5.61681800e-02  1.84735041e-02 -4.64849770e-02\n",
      "  2.84189079e-02  3.88638377e-02 -5.78378320e-01 -1.98933601e-01\n",
      " -6.54578388e-01 -2.08898321e-01  1.61051422e-01  6.86282218e-02\n",
      " -1.86340764e-01  3.37304287e-02 -2.95705885e-01  2.56646574e-02\n",
      " -3.18203062e-01 -3.38543244e-02  8.69825259e-02 -1.65467247e-01\n",
      " -3.07513356e-01  7.43200555e-02  3.53462905e-01 -2.66905040e-01\n",
      " -1.82728902e-01 -1.87545475e-02  2.89208684e-02  4.23821896e-01\n",
      "  1.02494217e-01  9.41635147e-02  1.88466489e-01  2.29482055e-01\n",
      "  1.72587726e-02 -3.72008950e-01 -1.63045362e-01  3.00885346e-02\n",
      " -2.09647462e-01  1.80983827e-01 -7.19894990e-02  9.07604620e-02\n",
      "  8.39722529e-02  1.13654613e-01 -7.66589642e-02  2.93442339e-01\n",
      "  3.08566213e-01 -1.55912682e-01  3.44167143e-01  3.77856612e-01\n",
      "  3.86568516e-01 -3.83628696e-01  1.31506518e-01 -1.73217371e-01\n",
      "  1.63670719e-01 -2.05111623e-01 -6.38212338e-02  1.16026260e-01\n",
      " -3.53874952e-01 -3.87074679e-01 -4.59273100e-01  2.34443009e-01\n",
      "  1.95995376e-01  3.65464121e-01 -2.23028973e-01 -4.20063883e-01\n",
      "  1.90422371e-01  3.26164722e-01  4.22025919e-01 -8.81390199e-02\n",
      " -1.61260232e-01 -4.01840992e-02  6.38023555e-01  4.32747990e-01\n",
      "  2.12451398e-01 -8.03059936e-02 -1.53411970e-01 -4.26394105e-01\n",
      " -5.43821370e-03  3.04960996e-01  2.18625963e-01 -2.14495242e-01\n",
      "  6.57047182e-02  2.10268095e-01  6.97246373e-01  7.70652667e-02\n",
      "  2.52830535e-02 -4.43788886e-01 -1.65758044e-01 -2.12652832e-02\n",
      "  2.09813580e-01  1.56542256e-01  5.21759331e-01  1.50104836e-01\n",
      " -1.86978921e-01  3.28466684e-01  3.09583515e-01  7.74470493e-02\n",
      " -5.27825832e-01  2.59915981e-02  5.24905086e-01  3.38634998e-01\n",
      " -1.33824900e-01 -8.89088511e-02 -5.39221577e-02 -6.89869702e-01\n",
      "  3.87615524e-02 -1.30642578e-01  1.48401842e-01  3.35592508e-01\n",
      "  7.05927610e-03  8.05654675e-02  4.37699445e-02 -6.18154824e-01\n",
      " -1.73192248e-01  9.72514153e-02  1.19350098e-01 -4.20425206e-01\n",
      "  7.29408205e-01 -3.67242247e-01 -9.37322080e-02  2.26034924e-01\n",
      " -4.15042676e-02 -1.46037057e-01  5.24536192e-01  8.73014554e-02\n",
      " -2.96411309e-02 -3.59353334e-01  4.04071994e-02 -1.07486896e-01\n",
      "  3.11743647e-01 -2.24270537e-01 -5.61840832e-01 -1.39995158e-01\n",
      "  8.14239203e-04 -2.89665520e-01  6.20135665e-02  2.02241540e-02\n",
      " -1.26386620e-02 -2.48153493e-01  2.38769487e-01  2.16282129e-01\n",
      "  4.88649821e-03 -4.52352762e-02  1.46446764e-01 -5.37910759e-01\n",
      " -2.98654437e-01  1.60891023e-02 -3.01470518e-01 -5.93786351e-02]\n"
     ]
    }
   ],
   "source": [
    "vector = ft_model[ft_model.key_to_index['работа']]\n",
    "print(vector)\n",
    "# w2v_model.save('fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea89fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = w2v_model[:]\n",
    "# X = word_averaging_list(ft_model, X_token)\n",
    "\n",
    "X_train = word_averaging_list_pre_trained(ft_model, X_train_token_pre_trained)\n",
    "X_test = word_averaging_list_pre_trained(ft_model, X_test_token_pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a48d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b752",
   "metadata": {},
   "source": [
    "## Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5f43d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.8183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "------------------------------\n",
      "tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f1_score_ft_pre_trained = []\n",
    "\n",
    "for layers in [[16, 8, 4], [16, 16, 16], [32, 16, 8, 4], [32, 32, 32, 32], [64, 32, 16, 8, 4], [64, 64, 64, 64, 64]]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:   \n",
    "        net = None\n",
    "        if len(layers) - 1 == 2:\n",
    "            net = Net_2_layer(layers=layers)\n",
    "        elif len(layers) - 1 == 3:\n",
    "            net = Net_3_layer(layers=layers)\n",
    "        else:\n",
    "            net = Net_4_layer(layers=layers)\n",
    "\n",
    "        net = train_net(net=net, learning_rate=lr, x=X_train, y=y_train, batch=1, epochs=EPOCHS, device=device)\n",
    "        f1_score_ft_pre_trained.append(test_net(net=net, device=device, x=X_test, y=y_test, batch=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "868ccc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-March-2023 14:40:55\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime('%d-%B-%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c97b",
   "metadata": {},
   "source": [
    "# Сводная таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f381268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Количество скрытых слоев</th>\n",
       "      <th>Количество нейронов</th>\n",
       "      <th>Скорость обучения</th>\n",
       "      <th>Weighted F1-score self-trained W2V</th>\n",
       "      <th>Weighted F1-score self-trained FT</th>\n",
       "      <th>Weighted F1-score pre-trained W2V</th>\n",
       "      <th>Weighted F1-score pre-trained FT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>16-16</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>32-16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>32-32-32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>64-32-16-8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>64-64-64-64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Количество скрытых слоев Количество нейронов Скорость обучения  \\\n",
       "0                          2                16-8              0.01   \n",
       "1                          2                16-8             0.001   \n",
       "2                          2                16-8            0.0001   \n",
       "3                          2               16-16              0.01   \n",
       "4                          2               16-16             0.001   \n",
       "5                          2               16-16            0.0001   \n",
       "6                          3             32-16-8              0.01   \n",
       "7                          3             32-16-8             0.001   \n",
       "8                          3             32-16-8            0.0001   \n",
       "9                          3            32-32-32              0.01   \n",
       "10                         3            32-32-32             0.001   \n",
       "11                         3            32-32-32            0.0001   \n",
       "12                         4          64-32-16-8              0.01   \n",
       "13                         4          64-32-16-8             0.001   \n",
       "14                         4          64-32-16-8            0.0001   \n",
       "15                         4         64-64-64-64              0.01   \n",
       "16                         4         64-64-64-64             0.001   \n",
       "17                         4         64-64-64-64            0.0001   \n",
       "\n",
       "    Weighted F1-score self-trained W2V  Weighted F1-score self-trained FT  \\\n",
       "0                                 0.82                               0.38   \n",
       "1                                 0.76                               0.38   \n",
       "2                                 0.38                               0.38   \n",
       "3                                 0.82                               0.80   \n",
       "4                                 0.70                               0.74   \n",
       "5                                 0.38                               0.38   \n",
       "6                                 0.75                               0.79   \n",
       "7                                 0.38                               0.38   \n",
       "8                                 0.38                               0.38   \n",
       "9                                 0.82                               0.82   \n",
       "10                                0.38                               0.38   \n",
       "11                                0.38                               0.38   \n",
       "12                                0.38                               0.80   \n",
       "13                                0.38                               0.38   \n",
       "14                                0.38                               0.38   \n",
       "15                                0.77                               0.84   \n",
       "16                                0.38                               0.38   \n",
       "17                                0.38                               0.38   \n",
       "\n",
       "    Weighted F1-score pre-trained W2V  Weighted F1-score pre-trained FT  \n",
       "0                                0.73                              0.58  \n",
       "1                                0.70                              0.38  \n",
       "2                                0.38                              0.38  \n",
       "3                                0.75                              0.50  \n",
       "4                                0.70                              0.38  \n",
       "5                                0.38                              0.38  \n",
       "6                                0.74                              0.38  \n",
       "7                                0.38                              0.38  \n",
       "8                                0.38                              0.38  \n",
       "9                                0.72                              0.38  \n",
       "10                               0.69                              0.38  \n",
       "11                               0.38                              0.38  \n",
       "12                               0.68                              0.38  \n",
       "13                               0.38                              0.38  \n",
       "14                               0.38                              0.38  \n",
       "15                               0.70                              0.38  \n",
       "16                               0.38                              0.38  \n",
       "17                               0.38                              0.38  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame()\n",
    "summary['Количество скрытых слоев'] = np.array([2] * 6 + [3] * 6 + [4] * 6, dtype=int)\n",
    "summary['Количество нейронов'] = np.array(['16-8'] * 3 + ['16-16'] * 3 + ['32-16-8'] * 3 + ['32-32-32'] * 3 + ['64-32-16-8'] * 3 + ['64-64-64-64'] * 3, dtype=str)\n",
    "summary['Скорость обучения'] = np.array(['0.01', '0.001', '0.0001'] * 6, dtype=str)\n",
    "summary['Weighted F1-score self-trained W2V'] = np.array([round(el, 2) for el in f1_score_w2v_self_trained], dtype=float)\n",
    "summary['Weighted F1-score self-trained FT'] = np.array([round(el, 2) for el in f1_score_ft_self_trained], dtype=float)\n",
    "summary['Weighted F1-score pre-trained W2V'] = np.array([round(el, 2) for el in f1_score_w2v_pre_trained], dtype=float)\n",
    "summary['Weighted F1-score pre-trained FT'] = np.array([round(el, 2) for el in f1_score_ft_pre_trained], dtype=float)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3dec8",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1668b5a",
   "metadata": {},
   "source": [
    "Эксперименты показали высокую точность нейросетей. Лучший результат был достигнут на самостоятельно обученной FastText модели на 4 слойной сети с одинаковыми слоями по 64 нейрона в каждом с показателем f1-weighted = 0.84. Обучение выполнялось с batch_size = 1, SDG и 10 эпохах. В целом, модели Word2Vec показали себя более устойчивыми к обучаемости и в среднем результат показан получше, чем у FastText."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P75gWMshhfv_",
    "_Q6YfGFAgyW-",
    "pZTCtGBdmPCN",
    "WKGUJrpWmbNX",
    "jcrq3YYfmdth",
    "Tv9-ok4Qmhb6",
    "JRiPVfwVOOdY",
    "MSvRndCKo9nM",
    "lSadA7atpSP2",
    "YX47aVQ7pbVj",
    "Sz1ERkCEpetp",
    "kYbqQt_zpgjL",
    "B5C1IynVf7Ry"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
